{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911b3b37-3b29-4833-94f2-bfe47af00c83",
   "metadata": {},
   "source": [
    "# Lab 6: Essay Writer\n",
    "\n",
    "## Setup and Imports\n",
    "\n",
    "In this section, we're building a more complex project: an AI Essay Writer. We'll start by setting up our environment and importing the necessary libraries. We're using Amazon Bedrock and Anthropic's Claude model, so our imports reflect that.\n",
    "We're setting up logging, configuring Bedrock, and retrieving our Tavily API key. Tavily is a research tool we'll use to gather information for our essays. Make sure you have your Tavily API key stored securely.\n",
    "\n",
    "A few requirements for the UI that we build at the end:\n",
    "\n",
    "1. Add your tavily ai key to the `.env` file.\n",
    "\n",
    "2. Pay attention that you run on LangGraph 0.0.53.\n",
    "\n",
    "3. Run the helper.py from the CLI and open the web browser to step through the graph or run it in the notebook directly!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5762271-8736-4e94-9444-8c92bd0e8074",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import json, re\n",
    "import pprint\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "\n",
    "# import local modules\n",
    "dir_current = os.path.abspath(\"\")\n",
    "dir_parent = os.path.dirname(dir_current)\n",
    "if dir_parent not in sys.path:\n",
    "    sys.path.append(dir_parent)\n",
    "from utils import utils\n",
    "\n",
    "# Set basic configs\n",
    "logger = utils.set_logger()\n",
    "pp = utils.set_pretty_printer()\n",
    "\n",
    "# Load environment variables from .env file or Secret Manager\n",
    "_ = load_dotenv(\"../.env\")\n",
    "aws_region = os.getenv(\"AWS_REGION\")\n",
    "tavily_ai_api_key = utils.get_tavily_api(\"TAVILY_API_KEY\", aws_region)\n",
    "\n",
    "# Set bedrock configs\n",
    "bedrock_config = Config(\n",
    "    connect_timeout=120, read_timeout=120, retries={\"max_attempts\": 0}\n",
    ")\n",
    "\n",
    "# Create a bedrock runtime client\n",
    "bedrock_rt = boto3.client(\n",
    "    \"bedrock-runtime\", region_name=aws_region, config=bedrock_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252ba3e5",
   "metadata": {},
   "source": [
    "## Defining the Agent State\n",
    "\n",
    "Now, let's define our agent's state. This is more complex than in previous lessons because our essay writer has multiple steps and needs to keep track of various pieces of information.\n",
    "\n",
    "We're creating a TypedDict called AgentState. It includes:\n",
    "\n",
    "- task: The essay topic or question\n",
    "- plan: The outline of the essay\n",
    "- draft: The current version of the essay\n",
    "- critique: Feedback on the current draft\n",
    "- content: Research information from Tavily\n",
    "- revision_number: How many revisions we've made\n",
    "- max_revisions: The maximum number of revisions we want to make\n",
    "\n",
    "These elements will help us manage the essay writing process and know when to stop revising.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0168aee-bce9-4d60-b827-f86a88187e31",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AnyMessage,\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    "    ChatMessage,\n",
    ")\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589c5b6-6cc2-4594-9a17-dccdcf676054",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aac3dc",
   "metadata": {},
   "source": [
    "## Setting up the Model\n",
    "\n",
    "We're using Anthropic's Claude model via Amazon Bedrock. We're setting it up with a temperature of 0 for more consistent outputs. The model we're using is claude-3-haiku, which is well-suited for this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba84ec-c172-4de7-ac55-e3158a531b23",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "model = ChatBedrockConverse(\n",
    "    client=bedrock_rt,\n",
    "    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d61f1e5",
   "metadata": {},
   "source": [
    "## Defining Prompts\n",
    "\n",
    "Our essay writer uses several prompts for different stages of the process:\n",
    "\n",
    "1. **PLAN_PROMPT**: This instructs the model to create an essay outline.\n",
    "2. **WRITER_PROMPT**: This guides the model in writing the essay based on the plan and research.\n",
    "3. **REFLECTION_PROMPT**: This tells the model how to critique the essay.\n",
    "4. **RESEARCH_PLAN_PROMPT** and **RESEARCH_CRITIQUE_PROMPT**: These help generate search queries for our research step.\n",
    "\n",
    "Each prompt is carefully crafted to guide the model in performing its specific task within the essay writing process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d5092-b8ef-4e38-b4d7-0e80c609bf7a",
   "metadata": {
    "height": 74
   },
   "outputs": [],
   "source": [
    "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay. \\\n",
    "Write such an outline for the user provided topic. Give an outline of the essay along with any relevant notes \\\n",
    "or instructions for the sections.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10084a02-2928-4945-9f7c-ad3f5b33caf7",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "WRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 5-paragraph essays.\\\n",
    "Generate the best essay possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of your previous attempts. \\\n",
    "Utilize all the information below as needed: \n",
    "\n",
    "------\n",
    "<content>\n",
    "{content}\n",
    "</content>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d1205-f8fc-4912-b148-2a45da99219c",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83588e70-254f-4f83-a510-c8ae81e729b0",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when writing the following essay. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3ef4c-58b3-401b-b104-0d51e553d982",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as outlined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. Only generate 3 queries max.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9c562",
   "metadata": {},
   "source": [
    "### Excursion on strucutred output generation with Anthropic Models:\n",
    "\n",
    "Take a look at the prompts above.\n",
    "\n",
    "- Do they follow the prompting guide of Anthropic?\n",
    "- Do you think you could get more consistent outputs by requesting an answer structure that would look something like this, e.g. for the REFLECTION_PROMPT:\n",
    "\n",
    "```xml\n",
    "<answer>\n",
    "  <overall_assessment>\n",
    "    <strengths>\n",
    "      <strength_point></strength_point>\n",
    "      <strength_point></strength_point>\n",
    "    </strengths>\n",
    "    <weaknesses>\n",
    "      <weakness_point></weakness_point>\n",
    "      <weakness_point></weakness_point>\n",
    "    </weaknesses>\n",
    "  </overall_assessment>\n",
    "\n",
    "...\n",
    "\n",
    "  <style_and_language>\n",
    "    <clarity>\n",
    "      <comment></comment>\n",
    "      <recommendation></recommendation>\n",
    "    </clarity>\n",
    "    <tone>\n",
    "      <comment></comment>\n",
    "      <recommendation></recommendation>\n",
    "    </tone>\n",
    "    <grammar_and_mechanics>\n",
    "      <comment></comment>\n",
    "      <recommendation></recommendation>\n",
    "    </grammar_and_mechanics>\n",
    "  </style_and_language>\n",
    "\n",
    "  <length_assessment>\n",
    "    <comment></comment>\n",
    "    <recommendation></recommendation>\n",
    "  </length_assessment>\n",
    "\n",
    "  <conclusion>\n",
    "    <overall_recommendation></overall_recommendation>\n",
    "    <priority_improvements>\n",
    "      <improvement></improvement>\n",
    "      <improvement></improvement>\n",
    "    </priority_improvements>\n",
    "  </conclusion>\n",
    "</answer>\n",
    "```\n",
    "\n",
    "- What would the advantage and disadvantage of such a structure be?\n",
    "- Ask yourself, if the extra tokens are worth it? When should you invest in a detailed, token intensive prompt, vs. a more freeform one?\n",
    "\n",
    "- How would you parse this output?\n",
    "\n",
    "**Hint:**\n",
    "You can combine XMLOutput-Parser from LangChain and PyDantic models.\n",
    "\n",
    "For reference, below you can find how to use the XMLOutput parser, without any dependencies on methods like `.with_structured_output(...)`, which is a very recent addition to langchain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff3ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# Create the XMLOutputParser with our Pydantic model\n",
    "essay_critique_parser = XMLOutputParser()\n",
    "\n",
    "# Example usage\n",
    "xml_string = \"\"\"\n",
    "<answer>\n",
    "  <overall_assessment>\n",
    "    <strengths>\n",
    "      <strength_point>Clear thesis statement</strength_point>\n",
    "      <strength_point>Well-structured paragraphs</strength_point>\n",
    "    </strengths>\n",
    "    <weaknesses>\n",
    "      <weakness_point>Lack of detailed examples</weakness_point>\n",
    "      <weakness_point>Some grammatical errors</weakness_point>\n",
    "    </weaknesses>\n",
    "  </overall_assessment>\n",
    "  <content_evaluation>\n",
    "    <depth_of_analysis>\n",
    "      <comment>The analysis lacks depth in some areas.</comment>\n",
    "      <recommendation>Expand on key points with more detailed explanations.</recommendation>\n",
    "    </depth_of_analysis>\n",
    "    <argument_quality>\n",
    "      <comment>Arguments are logical but could be stronger.</comment>\n",
    "      <recommendation>Provide more evidence to support your claims.</recommendation>\n",
    "    </argument_quality>\n",
    "    <evidence_use>\n",
    "      <comment>Limited use of supporting evidence.</comment>\n",
    "      <recommendation>Incorporate more relevant examples and data.</recommendation>\n",
    "    </evidence_use>\n",
    "  </content_evaluation>\n",
    "  <structure_and_organization>\n",
    "    <comment>The essay has a clear structure but transitions could be improved.</comment>\n",
    "    <recommendation>Work on smoother transitions between paragraphs.</recommendation>\n",
    "  </structure_and_organization>\n",
    "  <style_and_language>\n",
    "    <clarity>\n",
    "      <comment>Writing is generally clear but some sentences are convoluted.</comment>\n",
    "      <recommendation>Simplify complex sentences for better readability.</recommendation>\n",
    "    </clarity>\n",
    "    <tone>\n",
    "      <comment>The tone is appropriate for an academic essay.</comment>\n",
    "      <recommendation>Maintain this formal tone throughout.</recommendation>\n",
    "    </tone>\n",
    "    <grammar_and_mechanics>\n",
    "      <comment>There are a few grammatical errors and typos.</comment>\n",
    "      <recommendation>Proofread carefully to eliminate these errors.</recommendation>\n",
    "    </grammar_and_mechanics>\n",
    "  </style_and_language>\n",
    "  <length_assessment>\n",
    "    <comment>The essay meets the required length.</comment>\n",
    "    <recommendation>No changes needed in terms of length.</recommendation>\n",
    "  </length_assessment>\n",
    "  <conclusion>\n",
    "    <overall_recommendation>This is a solid essay that could be improved with more depth and better proofreading.</overall_recommendation>\n",
    "    <priority_improvements>\n",
    "      <improvement>Deepen analysis with more detailed explanations and examples.</improvement>\n",
    "      <improvement>Carefully proofread to eliminate grammatical errors and typos.</improvement>\n",
    "    </priority_improvements>\n",
    "  </conclusion>\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "# Parse the XML string\n",
    "parsed_critique = essay_critique_parser.parse(xml_string)\n",
    "parsed_critique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d75275",
   "metadata": {},
   "source": [
    "However, we can also use `.with_structured_output` to get the answer we need!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cefa30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class StructuredOutput(BaseModel):\n",
    "    title: str = Field(..., description=\"The title of the response\")\n",
    "    content: str = Field(..., description=\"The main content of the response\")\n",
    "    summary: str = Field(..., description=\"A brief summary of the content\")\n",
    "\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    model_kwargs={\"temperature\": 0},\n",
    ")\n",
    "\n",
    "structured_llm = llm.with_structured_output(StructuredOutput)\n",
    "\n",
    "response = structured_llm.invoke(\"Tell me about artificial intelligence\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7c615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(\n",
    "    StructuredOutput, method=\"xml_mode\"\n",
    ")  # try xml_mode\n",
    "response = structured_llm.invoke(\"Tell me about artificial intelligence\")\n",
    "print(f\"The title:\\t{response.title}\\n\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3293b7-a50c-43c8-a022-8975e1e444b8",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfafd4a",
   "metadata": {},
   "source": [
    "### Setting up Tavily Client\n",
    "\n",
    "We're using the Tavily API for research. We import the TavilyClient and initialize it with our API key. This will allow us to perform web searches to gather information for our essays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722c3d4-4cbf-43bf-81b0-50f634c4ce61",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "import os\n",
    "\n",
    "tavily = TavilyClient(api_key=tavily_ai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4114e17",
   "metadata": {},
   "source": [
    "## Defining Node Functions\n",
    "\n",
    "Now we're creating the individual components of our essay writing process. Each function represents a node in our graph:\n",
    "\n",
    "1. plan_node: Creates the essay outline.\n",
    "2. research_plan_node: Generates search queries and fetches information based on the plan.\n",
    "3. generation_node: Writes the essay draft.\n",
    "4. reflection_node: Critiques the current draft.\n",
    "5. research_critique_node: Performs additional research based on the critique.\n",
    "6. should_continue: Decides whether to continue revising or stop.\n",
    "\n",
    "Each of these functions interacts with our Claude model and updates the agent's state accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f82fe-3ec4-4917-be51-9fb10d1317fa",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "def plan_node(state: AgentState):\n",
    "    messages = [SystemMessage(content=PLAN_PROMPT), HumanMessage(content=state[\"task\"])]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"plan\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ee9b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import json\n",
    "\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[str] = Field(description=\"List of research queries\")\n",
    "\n",
    "\n",
    "def research_plan_node(state: AgentState):\n",
    "    # Set up the Pydantic output parser\n",
    "    parser = PydanticOutputParser(pydantic_object=Queries)\n",
    "\n",
    "    # Create a prompt template with format instructions\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"Generate research queries based on the given task.\\n{format_instructions}\\nTask: {task}\\n\",\n",
    "        input_variables=[\"task\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Use the model with the new prompt and parser\n",
    "    queries_output = model.invoke(prompt.format_prompt(task=state[\"task\"]))\n",
    "\n",
    "    # Extract the content from the AIMessage\n",
    "    queries_text = queries_output.content\n",
    "\n",
    "    # Extract the JSON string from the content\n",
    "    json_start = queries_text.find(\"{\")\n",
    "    json_end = queries_text.rfind(\"}\") + 1\n",
    "    json_str = queries_text[json_start:json_end]\n",
    "\n",
    "    # Parse the JSON string\n",
    "    queries_dict = json.loads(json_str)\n",
    "\n",
    "    # Create a Queries object from the parsed JSON\n",
    "    parsed_queries = Queries(**queries_dict)\n",
    "\n",
    "    content = state[\"content\"] or []\n",
    "    for q in parsed_queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response[\"results\"]:\n",
    "            content.append(r[\"content\"])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f303b1-a4d0-408c-8cc0-515ff980717f",
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join(state[\"content\"] or [])\n",
    "    user_message = HumanMessage(\n",
    "        content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\"\n",
    "    )\n",
    "    messages = [\n",
    "        SystemMessage(content=WRITER_PROMPT.format(content=content)),\n",
    "        user_message,\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\n",
    "        \"draft\": response.content,\n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4dcb93-6298-4cfd-b3ce-61dfac7fb35f",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT),\n",
    "        HumanMessage(content=state[\"draft\"]),\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"critique\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27526845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_critique_node(state: AgentState):\n",
    "    # Set up the Pydantic output parser\n",
    "    parser = PydanticOutputParser(pydantic_object=Queries)\n",
    "\n",
    "    # Create a prompt template with format instructions\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"Generate research queries based on the given critique.\\n{format_instructions}\\nCritique: {critique}\\n\",\n",
    "        input_variables=[\"critique\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Use the model with the new prompt and parser\n",
    "    queries_output = model.invoke(prompt.format_prompt(critique=state[\"critique\"]))\n",
    "\n",
    "    # Extract the content from the AIMessage\n",
    "    queries_text = queries_output.content\n",
    "\n",
    "    # Extract the JSON string from the content\n",
    "    json_start = queries_text.find(\"{\")\n",
    "    json_end = queries_text.rfind(\"}\") + 1\n",
    "    json_str = queries_text[json_start:json_end]\n",
    "\n",
    "    # Parse the JSON string\n",
    "    queries_dict = json.loads(json_str)\n",
    "\n",
    "    # Create a Queries object from the parsed JSON\n",
    "    parsed_queries = Queries(**queries_dict)\n",
    "\n",
    "    content = state[\"content\"] or []\n",
    "    for q in parsed_queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response[\"results\"]:\n",
    "            content.append(r[\"content\"])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff362f49-dcf1-4ea1-a86c-e516e9ab897d",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "def should_continue(state):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7044a487",
   "metadata": {},
   "source": [
    "## Building the Graph\n",
    "\n",
    "With our nodes defined, we can now build our graph. We're using LangGraph's StateGraph to create a flow for our essay writing process. We add each node to the graph, set the entry point to the planner, and define the edges between nodes.\n",
    "\n",
    "The key part here is the conditional edge after the generate node. It uses our should_continue function to decide whether to reflect and revise, or to end the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e15a20-83d7-434c-8551-bce8dcc32be0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "builder = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab2c74-f32e-490c-a85d-932d11444210",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833d3ce-bd31-4319-811d-decff226b970",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "builder.set_entry_point(\"planner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e93cce-6eab-4c7c-ac64-e9993fdb30d6",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "builder.add_conditional_edges(\n",
    "    \"generate\", should_continue, {END: END, \"reflect\": \"reflect\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d0990-a932-423f-9ff3-5cada58c5f32",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cde654-64e2-48bc-80a9-0ed668ccb7dc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871f644-b131-4065-b7ce-b82c20a41f11",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392cfcff",
   "metadata": {},
   "source": [
    "## Running the Graph\n",
    "\n",
    "To test our essay writer, we're using the graph.stream method. This allows us to see each step of the process as it happens. We're asking it to write an essay on the difference between LangChain and LangSmith, with a maximum of two revisions.\n",
    "\n",
    "As it runs, you'll see outputs from each node, showing you how the essay evolves through the planning, research, writing, and revision stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f3be1d-cc4c-41fa-9863-3e386e88e305",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for s in graph.stream(\n",
    "    {\n",
    "        \"task\": \"what is the difference between langchain and langsmith\",\n",
    "        \"max_revisions\": 2,\n",
    "        \"revision_number\": 1,\n",
    "    },\n",
    "    thread,\n",
    "):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1664b5-75e0-46b7-9c2b-4ac9171f4597",
   "metadata": {},
   "source": [
    "## Essay Writer Interface\n",
    "\n",
    "Finally, lets use a simple GUI using Gradio to make it easy to interact with our essay writer. \n",
    "\n",
    "**IMPORTANT NOTE**: To use Gradio within the Amazon SageMaker Code Editor, the app needs to be launched in `shared=True` mode, which creates a public link. Review the [Security and File Access](https://www.gradio.app/guides/sharing-your-app#security-and-file-access) to make sure you understand the security implications.\n",
    "\n",
    "This GUI allows you to input an essay topic, generate the essay, and see the results of each step in the process. You can also interrupt the process after each step, view the current state of the essay, and even modify the topic or plan if you want to guide the essay in a different direction.\n",
    "\n",
    "This GUI makes it easy to experiment with the essay writer and see how changes to the input or the process affect the final output.\n",
    "\n",
    "And that concludes our AI Essay Writer project! You now have a complex, multi-step AI agent that can research, write, and refine essays on a wide range of topics. This project demonstrates how you can combine different AI and API services to create a powerful, practical application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8a6cc-65d4-4ce7-87aa-4e67d7c23d7b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "#set magic variables to allow for a reload when changing code without restarting the kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gradio as gr\n",
    "from helper import ewriter, writer_gui\n",
    "\n",
    "MultiAgent = ewriter()\n",
    "app = writer_gui(MultiAgent.graph)\n",
    "app.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37002eb0",
   "metadata": {},
   "source": [
    "## Exercise - Rewrite the Essay writers prompts and parsers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b56d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e21704cb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents-dev-env",
   "language": "python",
   "name": "agents-dev-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

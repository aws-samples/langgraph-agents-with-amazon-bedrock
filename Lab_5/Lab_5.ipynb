{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad976db-fb87-4a92-b0d2-06defc098339",
   "metadata": {},
   "source": [
    "# Lab 5: Human in the Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9644a30b",
   "metadata": {},
   "source": [
    "## Setup and Initialization\n",
    "\n",
    "We start by setting up our environment and importing necessary modules. A crucial component is the checkpointer:\n",
    "\n",
    "```python\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "```\n",
    "\n",
    "The checkpointer functions similarly to a distributed version control system like Git, but for AI states. It allows us to create \"commits\" of our AI's state at different points in its decision-making process, and \"branch\" or \"revert\" as needed.\n",
    "\n",
    "Be aware that excessive use of check-pointing can lead to significant memory usage, especially with large state objects. Implement a strategy to manage or expire old checkpoints in long-running applications. Think of usage of Time-To-Live for those checkpoints to be implemented in your database, if you no longer have a need for those checkpoints.\n",
    "We will be using SQLite as our database of choice for our checkpoints, but you might want to switch to either Redis or Postgres for your production applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5762271-8736-4e94-9444-8c92bd0e8074",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import json, re\n",
    "import pprint\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "\n",
    "# import local modules\n",
    "dir_current = os.path.abspath(\"\")\n",
    "dir_parent = os.path.dirname(dir_current)\n",
    "if dir_parent not in sys.path:\n",
    "    sys.path.append(dir_parent)\n",
    "from utils import utils\n",
    "\n",
    "bedrock_config = Config(\n",
    "    connect_timeout=120, read_timeout=120, retries={\"max_attempts\": 0}\n",
    ")\n",
    "\n",
    "# Set basic configs\n",
    "logger = utils.set_logger()\n",
    "pp = utils.set_pretty_printer()\n",
    "\n",
    "# Load environment variables from .env file or Secret Manager\n",
    "_ = load_dotenv(\"../.env\")\n",
    "aws_region = os.getenv(\"AWS_REGION\")\n",
    "tavily_ai_api_key = utils.get_tavily_api(\"TAVILY_API_KEY\", aws_region)\n",
    "\n",
    "# Set bedrock configs\n",
    "bedrock_config = Config(\n",
    "    connect_timeout=120, read_timeout=120, retries={\"max_attempts\": 0}\n",
    ")\n",
    "\n",
    "# Create a bedrock runtime client\n",
    "bedrock_rt = boto3.client(\n",
    "    \"bedrock-runtime\", region_name=aws_region, config=bedrock_config\n",
    ")\n",
    "\n",
    "# Create a bedrock client to check available models\n",
    "bedrock = boto3.client(\"bedrock\", region_name=aws_region, config=bedrock_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0168aee-bce9-4d60-b827-f86a88187e31",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c04fb",
   "metadata": {},
   "source": [
    "Next, we set up our agent state with a custom message handling function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589c5b6-6cc2-4594-9a17-dccdcf676054",
   "metadata": {
    "height": 557
   },
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "\"\"\"\n",
    "In previous examples we've annotated the `messages` state key\n",
    "with the default `operator.add` or `+` reducer, which always\n",
    "appends new messages to the end of the existing messages array.\n",
    "\n",
    "Now, to support replacing existing messages, we annotate the\n",
    "`messages` key with a customer reducer function, which replaces\n",
    "messages with the same `id`, and appends them otherwise.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def reduce_messages(\n",
    "    left: list[AnyMessage], right: list[AnyMessage]\n",
    ") -> list[AnyMessage]:\n",
    "    # assign ids to messages that don't have them\n",
    "    for message in right:\n",
    "        if not message.id:\n",
    "            message.id = str(uuid4())\n",
    "    # merge the new messages with the existing messages\n",
    "    merged = left.copy()\n",
    "    for message in right:\n",
    "        for i, existing in enumerate(merged):\n",
    "            # replace any existing messages with the same id\n",
    "            if existing.id == message.id:\n",
    "                merged[i] = message\n",
    "                break\n",
    "        else:\n",
    "            # append any new messages to the end\n",
    "            merged.append(message)\n",
    "    return merged\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], reduce_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f864ced5",
   "metadata": {},
   "source": [
    "This function acts like a smart message broker in a microservices architecture. It ensures that our conversation history remains consistent and up-to-date, crucial for maintaining context in long-running AI interactions.\n",
    "\n",
    "## Tool and Agent Setup\n",
    "\n",
    "We integrate the Tavily search tool:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba84ec-c172-4de7-ac55-e3158a531b23",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58320dc-c542-4a3c-bcf1-088909f9b075",
   "metadata": {},
   "source": [
    "## Human Intervention in the Agent Class\n",
    "\n",
    "The `Agent` class implements several key features that enable human intervention in the AI's decision-making process:\n",
    "\n",
    "1. **Interrupt Before Action**:\n",
    "\n",
    "   ```python\n",
    "   self.graph = graph.compile(\n",
    "       checkpointer=checkpointer, interrupt_before=[\"action\"]\n",
    "   )\n",
    "   ```\n",
    "\n",
    "   This line sets up an interrupt before the \"action\" node. It allows human oversight before any action is taken, providing an opportunity for review or modification.\n",
    "   You can add interrupts before any node you wish for as we will see in the next lab.\n",
    "\n",
    "2. **State Examination**:\n",
    "   The `exists_action` method prints the current state:\n",
    "\n",
    "   ```python\n",
    "   def exists_action(self, state: AgentState):\n",
    "       print(state)\n",
    "       # ...\n",
    "   ```\n",
    "\n",
    "   This allows humans to inspect the AI's current state, including its reasoning and intended actions.\n",
    "\n",
    "3. **Action Visibility**:\n",
    "   In the `take_action` method, each tool call is printed:\n",
    "\n",
    "   ```python\n",
    "   print(f\"Calling: {t}\")\n",
    "   ```\n",
    "\n",
    "   This provides visibility into what actions the AI is about to take, allowing for potential human intervention.\n",
    "\n",
    "4. **Checkpointing**:\n",
    "   The `checkpointer` parameter in the constructor allows for saving and loading states. This enables \"time travel\" functionality, where humans can revisit and potentially modify previous decision points.\n",
    "\n",
    "5. **Modifiable State**:\n",
    "   The `AgentState` is a mutable structure. While not shown in this class directly, it allows for state modification, enabling humans to alter the AI's context or decisions.\n",
    "\n",
    "These features collectively create a framework for human-in-the-loop AI, where human operators can monitor, intervene, and guide the AI's decision-making process at critical junctures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a0e94e-d015-4106-b439-dbcd2fcb8bb0",
   "metadata": {
    "height": 642
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, system=\"\", checkpointer=None):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_bedrock)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\", self.exists_action, {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(\n",
    "            checkpointer=checkpointer, interrupt_before=[\"action\"]\n",
    "        )\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_bedrock(self, state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {\"messages\": [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        print(state)\n",
    "        result = state[\"messages\"][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t[\"name\"]].invoke(t[\"args\"])\n",
    "            results.append(\n",
    "                ToolMessage(tool_call_id=t[\"id\"], name=t[\"name\"], content=str(result))\n",
    "            )\n",
    "        print(\"Back to the model!\")\n",
    "        return {\"messages\": results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8320d5d6",
   "metadata": {},
   "source": [
    "The `interrupt_before=[\"action\"]` parameter implements a critical control point in our AI pipeline.\n",
    "\n",
    "It's analogous to implementing approval gates in a CI/CD pipeline, ensuring that no critical action is taken without necessary checks.\n",
    "\n",
    "To deepen your understanding of AI safety and control mechanisms, explore the paper [\"Concrete Problems in AI Safety\"](https://arxiv.org/abs/1606.06565) from researchers at Google, Stanford, and Berkeley from 2016.\n",
    "\n",
    "## Initializing and Running the Agent\n",
    "\n",
    "We initialize our agent with Claude on Amazon Bedrock:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10084a02-2928-4945-9f7c-ad3f5b33caf7",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "model = ChatBedrockConverse(\n",
    "    client=bedrock_rt,\n",
    "    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    ")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d1205-f8fc-4912-b148-2a45da99219c",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Whats the weather in Berlin?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4873049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c64a9cf",
   "metadata": {},
   "source": [
    "## Examining and Modifying State\n",
    "\n",
    "The ability to examine and modify the agent's state is a powerful feature:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83588e70-254f-4f83-a510-c8ae81e729b0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "abot.graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e23bc",
   "metadata": {},
   "source": [
    "This capability is similar to having a live debugger in a production environment, allowing you to inspect and modify the state of a running process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3ef4c-58b3-401b-b104-0d51e553d982",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "abot.graph.get_state(thread).next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f2735f",
   "metadata": {},
   "source": [
    "As you can see, the next node to be executed is the `('action',)` node.\n",
    "\n",
    "Remember, we compiled our graph with the interrupt set before the `action` node.\n",
    "\n",
    "```python\n",
    "        self.graph = graph.compile(\n",
    "            checkpointer=checkpointer, interrupt_before=[\"action\"]\n",
    "        )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f404d5-a3be-42c1-9990-b1e1ee011163",
   "metadata": {},
   "source": [
    "### ...continue after interrupt\n",
    "\n",
    "Now that stopped before the action (calling tavily-ai), we can continue. Lets see how that works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3293b7-a50c-43c8-a022-8975e1e444b8",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "for event in abot.graph.stream(None, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722c3d4-4cbf-43bf-81b0-50f634c4ce61",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "abot.graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f82fe-3ec4-4917-be51-9fb10d1317fa",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "abot.graph.get_state(thread).next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2da1c",
   "metadata": {},
   "source": [
    "### Adding your human input\n",
    "\n",
    "Next, we will see how to add your approval for executing the `action` to search the web.\n",
    "\n",
    "Depending if you are running in JupyterLab or in e.g. VS Code, the input box should pop up below the code cell, or on the top.\n",
    "\n",
    "![input box example, asking for user input](../assets/lab5_1.png)\n",
    "\n",
    "\n",
    "If you are fine with what will be searched, then please add a `y` for yes. Anything else, will abort the operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0fe1c7-77e2-499c-a2f9-1f739bb6ddf0",
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(\"Whats the weather in LA?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)\n",
    "\n",
    "while abot.graph.get_state(thread).next:\n",
    "    print(\"\\n\", abot.graph.get_state(thread), \"\\n\")\n",
    "    _input = input(\"proceed?\")\n",
    "    if _input != \"y\":\n",
    "        print(\"aborting\")\n",
    "        break\n",
    "    for event in abot.graph.stream(None, thread):\n",
    "        for v in event.values():\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1789a791",
   "metadata": {},
   "source": [
    "As you can see, the next node to be executed is the `('action',)` node.\n",
    "\n",
    "Remember, we compiled our graph with the interrupt set before the `action` node.\n",
    "\n",
    "```python\n",
    "        self.graph = graph.compile(\n",
    "            checkpointer=checkpointer, interrupt_before=[\"action\"]\n",
    "        )\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c5a21",
   "metadata": {},
   "source": [
    "Now one thing that can be very interesting, is to only stop and ask for human input, whenever you are calling a certain set of tools, and not all of them.\n",
    "\n",
    "Think about how that could be achieved, before continue reading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11891ceb",
   "metadata": {},
   "source": [
    "### Stop at only specific tool calls\n",
    "\n",
    "1. Parsing the tool call and stopping execution only if the tool call parameter `name` (tool name) is the same as as the stopping parameter.\n",
    "2. Adding all tools where you need to stop to an extra node.\n",
    "\n",
    "In general, option 2 is a cleaner and easier to debug. If you would like to see how this is implemented in a bigger example, please head over to the [customer support agent](https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/#part-3-conditional-interrupt) from the LangGraph examples to see how sensitive tools are handled.\n",
    "\n",
    "Here is a sneak peak of the graph:\n",
    "\n",
    "![customer support agent graph with sensitive and safe tools](../assets/lab5_2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbe5689-54ab-49ca-9055-6e5216abd523",
   "metadata": {},
   "source": [
    "## Modify State\n",
    "\n",
    "Run until the interrupt and then modify the state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f303b1-a4d0-408c-8cc0-515ff980717f",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(\"Whats the weather in LA?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4dcb93-6298-4cfd-b3ce-61dfac7fb35f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "abot.graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932883a4-c722-42bb-aec0-b4f41c5c81a4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "current_values = abot.graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff362f49-dcf1-4ea1-a86c-e516e9ab897d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "current_values.values[\"messages\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e15a20-83d7-434c-8551-bce8dcc32be0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "current_values.values[\"messages\"][-1].tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab2c74-f32e-490c-a85d-932d11444210",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "_id = current_values.values[\"messages\"][-1].tool_calls[0][\"id\"]\n",
    "current_values.values[\"messages\"][-1].tool_calls = [\n",
    "    {\n",
    "        \"name\": \"tavily_search_results_json\",\n",
    "        \"args\": {\"query\": \"current weather in Munich\"},\n",
    "        \"id\": _id,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833d3ce-bd31-4319-811d-decff226b970",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "abot.graph.update_state(thread, current_values.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e93cce-6eab-4c7c-ac64-e9993fdb30d6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "abot.graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d0990-a932-423f-9ff3-5cada58c5f32",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "for event in abot.graph.stream(None, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b870b-e0df-46f1-b29b-fb151ebcbcc3",
   "metadata": {},
   "source": [
    "## Time Travel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cde654-64e2-48bc-80a9-0ed668ccb7dc",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "states = []\n",
    "for state in abot.graph.get_state_history(thread):\n",
    "    print(state)\n",
    "    print(\"--\")\n",
    "    states.append(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449896c8-6ec6-4166-b640-9cd1530336a0",
   "metadata": {},
   "source": [
    "To fetch the same state as was filmed, the offset below is changed to `-3` from `-1`. This accounts for the initial state `__start__` and the first state that are now stored to state memory with the latest version of software.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871f644-b131-4065-b7ce-b82c20a41f11",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "to_replay = states[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3d8070-3f36-4cf0-a677-508e54359c8f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "to_replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f3be1d-cc4c-41fa-9863-3e386e88e305",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "for event in abot.graph.stream(None, to_replay.config):\n",
    "    for k, v in event.items():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005353dc-630b-4b40-ab19-56c57ac06611",
   "metadata": {},
   "source": [
    "## Go back in time and edit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8a6cc-65d4-4ce7-87aa-4e67d7c23d7b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "to_replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b5e62-a203-433c-92a0-3783f490cde1",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "_id = to_replay.values[\"messages\"][-1].tool_calls[0][\"id\"]\n",
    "to_replay.values[\"messages\"][-1].tool_calls = [\n",
    "    {\n",
    "        \"name\": \"tavily_search_results_json\",\n",
    "        \"args\": {\"query\": \"current weather in LA, accuweather\"},\n",
    "        \"id\": _id,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa923c-7e4f-42d1-965f-0f8ccd50fbd7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "branch_state = abot.graph.update_state(to_replay.config, to_replay.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c6245-2837-4ac5-983b-95f61f3ac10d",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "for event in abot.graph.stream(None, branch_state):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1016ce7-368d-4922-9044-fcdfd47c273b",
   "metadata": {},
   "source": [
    "## Add message to a state at a given time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b910915-b087-4d35-afff-0ec30a5852f1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "to_replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4feb6cc-5129-4a99-bb45-851bc07b5709",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "_id = to_replay.values[\"messages\"][-1].tool_calls[0][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85a02b4-96cc-4b01-8792-397a774eb499",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# Lets update the humidity to something that is impossible\n",
    "\n",
    "state_update = {\n",
    "    \"messages\": [\n",
    "        ToolMessage(\n",
    "            tool_call_id=_id,\n",
    "            name=\"tavily_search_results_json\",\n",
    "            content=\"23 degree celcius, 110 percent humidity\",\n",
    "        )\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b86a6-5e20-4252-b1d8-009b8318345a",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "branch_and_add = abot.graph.update_state(\n",
    "    to_replay.config, state_update, as_node=\"action\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af925917-b746-48c9-ac74-62fefbe5246c",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "for event in abot.graph.stream(None, branch_and_add):\n",
    "    for k, v in event.items():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b1de51-f1dd-4719-89e6-01c21d2b304e",
   "metadata": {},
   "source": [
    "# Extra Practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e06033-59fd-4d6b-8891-fb8bc2d7a037",
   "metadata": {},
   "source": [
    "## Build a small graph\n",
    "\n",
    "This is a small simple graph you can tinker with if you want more insight into controlling state memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7614683e-1c13-4518-b464-263fb91be761",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbef86a-75fe-416f-99ab-70b181d934dc",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c972f7-5a14-49da-9d82-37105a6637ed",
   "metadata": {},
   "source": [
    "Define a simple 2 node graph with the following state: -`lnode`: last node -`scratch`: a scratchpad location -`count` : a counter that is incremented each step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b846f637-5e98-4a7d-9ca0-5144302b7cef",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    lnode: str\n",
    "    scratch: str\n",
    "    count: Annotated[int, operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb1c6d-41b7-4f5b-807b-ecbdec82d8d6",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "def node1(state: AgentState):\n",
    "    print(f\"node1, count:{state['count']}\")\n",
    "    return {\n",
    "        \"lnode\": \"node_1\",\n",
    "        \"count\": 1,\n",
    "    }\n",
    "\n",
    "\n",
    "def node2(state: AgentState):\n",
    "    print(f\"node2, count:{state['count']}\")\n",
    "    return {\n",
    "        \"lnode\": \"node_2\",\n",
    "        \"count\": 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc403b-10d2-4a6c-bd21-c84bb8e0c8fc",
   "metadata": {},
   "source": [
    "The graph goes N1->N2->N1... but breaks after count reaches 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6d249-4793-41c5-8e80-40f1eedc4baf",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "def should_continue(state):\n",
    "    return state[\"count\"] < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3721d9-4508-48fa-9be0-93af20144072",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"Node1\", node1)\n",
    "builder.add_node(\"Node2\", node2)\n",
    "\n",
    "builder.add_edge(\"Node1\", \"Node2\")\n",
    "builder.add_conditional_edges(\"Node2\", should_continue, {True: \"Node1\", False: END})\n",
    "builder.set_entry_point(\"Node1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35d70c-daeb-49c1-9b7c-90cc2a7ca142",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57051f5-f7fb-4be5-a2f5-21cb1aa4fecb",
   "metadata": {},
   "source": [
    "### Run it!\n",
    "\n",
    "Now, set the thread and run!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edba3a1a-84a7-45eb-a6ee-74a612b68d54",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": str(1)}}\n",
    "graph.invoke({\"count\": 0, \"scratch\": \"hi\"}, thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7b035-bfdb-4642-8750-47d188277423",
   "metadata": {},
   "source": [
    "### Look at current state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43324904-a6f4-4299-86de-e0218e4cb225",
   "metadata": {},
   "source": [
    "Get the current state. Note the `values` which are the AgentState. Note the `config` and the `thread_ts`. You will be using those to refer to snapshots below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2513b41-31b2-46e6-84f6-0c519f697973",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7be4ae-24f0-4362-bd99-55ad38fe0112",
   "metadata": {},
   "source": [
    "View all the statesnapshots in memory. You can use the displayed `count` agentstate variable to help track what you see. Notice the most recent snapshots are returned by the iterator first. Also note that there is a handy `step` variable in the metadata that counts the number of steps in the graph execution. This is a bit detailed - but you can also notice that the _parent_config_ is the _config_ of the previous node. At initial startup, additional states are inserted into memory to create a parent. This is something to check when you branch or _time travel_ below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18647be0-c7c6-4ec5-9454-54a034fcd053",
   "metadata": {},
   "source": [
    "### Look at state history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686fa197-c97f-4ae6-82d2-938bbe5542c1",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "for state in graph.get_state_history(thread):\n",
    "    print(state, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7850ff-0748-43d7-8956-074fa9fd819f",
   "metadata": {},
   "source": [
    "Store just the `config` into an list. Note the sequence of counts on the right. `get_state_history` returns the most recent snapshots first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68e604-4f53-46c0-8f06-e7726ec9dcf6",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "states = []\n",
    "for state in graph.get_state_history(thread):\n",
    "    states.append(state.config)\n",
    "    print(state.config, state.values[\"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90790095-4080-4e76-b538-47caac7d9699",
   "metadata": {},
   "source": [
    "Grab an early state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1db574e-f158-44cf-b921-f1f4466c314d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "states[-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc233eb-f388-4ecd-bfb5-dac3568d37ce",
   "metadata": {},
   "source": [
    "This is the state after Node1 completed for the first time. Note `next` is `Node2`and `count` is 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f5039-7916-4c2a-88d5-d363b2898e70",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "graph.get_state(states[-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4872468e-4d23-4840-ae14-c06c1ab4f161",
   "metadata": {},
   "source": [
    "### Go Back in Time\n",
    "\n",
    "Use that state in `invoke` to go back in time. Notice it uses states[-3] as _current_state_ and continues to node2,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3049179-b901-4557-a9c4-78afb3d53d27",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "graph.invoke(None, states[-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf7789a-df3d-4c7e-8899-d96a99d45717",
   "metadata": {},
   "source": [
    "Notice the new states are now in state history. Notice the counts on the far right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa44ec-9bd1-484b-a415-cc5f50b6e799",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": str(1)}}\n",
    "for state in graph.get_state_history(thread):\n",
    "    print(state.config, state.values[\"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c8d305-6752-4cf6-a8cb-7babf3bbd643",
   "metadata": {},
   "source": [
    "You can see the details below. Lots of text, but try to find the node that start the new branch. Notice the parent _config_ is not the previous entry in the stack, but is the entry from state[-3].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d25697-fcf0-4a26-9485-3e195b0af225",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": str(1)}}\n",
    "for state in graph.get_state_history(thread):\n",
    "    print(state, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4653f91-761c-4185-8ca2-83de758308c7",
   "metadata": {},
   "source": [
    "### Modify State\n",
    "\n",
    "Let's start by starting a fresh thread and running to clean out history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf346c-ce22-4b6a-b18f-d200dfd991de",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "thread2 = {\"configurable\": {\"thread_id\": str(2)}}\n",
    "graph.invoke({\"count\": 0, \"scratch\": \"hi\"}, thread2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc1797-25e6-4931-b173-2e520b71c372",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ede9215-3133-4ad9-8dd5-6a8288ebe055",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "states2 = []\n",
    "for state in graph.get_state_history(thread2):\n",
    "    states2.append(state.config)\n",
    "    print(state.config, state.values[\"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab2c5f-cb1d-4851-8925-ce0af1f12a40",
   "metadata": {},
   "source": [
    "Start by grabbing a state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65313f6a-c7a9-49e3-aab2-0eb905e74a91",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "save_state = graph.get_state(states2[-3])\n",
    "save_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b354b-8c2d-4150-9150-ac9d8486af02",
   "metadata": {},
   "source": [
    "Now modify the values. One subtle item to note: Recall when agent state was defined, `count` used `operator.add` to indicate that values are _added_ to the current value. Here, `-3` will be added to the current count value rather than replace it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae6527b-912f-4f86-a07b-1fae684aaa77",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "save_state.values[\"count\"] = -3\n",
    "save_state.values[\"scratch\"] = \"hello\"\n",
    "save_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904bb1eb-2e39-472f-ae56-d8e95e2f1ab6",
   "metadata": {},
   "source": [
    "Now update the state. This creates a new entry at the _top_, or _latest_ entry in memory. This will become the current state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd78670-b119-45fa-934b-2ce2d477b4c2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "graph.update_state(thread2, save_state.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3beed-1c44-4364-bde9-b29ea5e8ca30",
   "metadata": {},
   "source": [
    "Current state is at the top. You can match the `thread_ts`.\n",
    "Notice the `parent_config`, `thread_ts` of the new node - it is the previous node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c7011-afe4-44bf-9e19-16220004912f",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "for i, state in enumerate(graph.get_state_history(thread2)):\n",
    "    if i >= 3:  # print latest 3\n",
    "        break\n",
    "    print(state, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d34cd8-99b6-4224-b345-fe8475f2a602",
   "metadata": {},
   "source": [
    "### Try again with `as_node`\n",
    "\n",
    "When writing using `update_state()`, you want to define to the graph logic which node should be assumed as the writer. What this does is allow th graph logic to find the node on the graph. After writing the values, the `next()` value is computed by travesing the graph using the new state. In this case, the state we have was written by `Node1`. The graph can then compute the next state as being `Node2`. Note that in some graphs, this may involve going through conditional edges! Let's try this out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfda2f6-5887-40fa-a733-a4357ad857d8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "graph.update_state(thread2, save_state.values, as_node=\"Node1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0788ac0f-5f64-49b2-b335-3094bbb19143",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "for i, state in enumerate(graph.get_state_history(thread2)):\n",
    "    if i >= 3:  # print latest 3\n",
    "        break\n",
    "    print(state, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8f52e-d1a6-46a0-a519-6fda0cb91624",
   "metadata": {},
   "source": [
    "`invoke` will run from the current state if not given a particular `thread_ts`. This is now the entry that was just added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9650d355-07c1-4a7d-a21d-4231673a7ee9",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "graph.invoke(None, thread2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59d3069-8fe7-4a05-9bfd-8e3a2e4590ad",
   "metadata": {},
   "source": [
    "Print out the state history, notice the `scratch` value change on the latest entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35088704-2b18-4d44-ba0e-dccbfde6ed9e",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "for state in graph.get_state_history(thread2):\n",
    "    print(state, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea1bba-cf12-4493-948b-08b300054742",
   "metadata": {},
   "source": [
    "Continue to experiment!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6782f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "199a41bb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents-dev-env",
   "language": "python",
   "name": "agents-dev-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
